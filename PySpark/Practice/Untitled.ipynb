{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44729569",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78737da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aac9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0117423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  3|4.0|string3|2000-03-01|2023-02-27 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2023, 2, 27, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "data.filter(datediff(current_timestamp(), data.e) < 7).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac755f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amt: float (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      "\n",
      "+---+------+------+----+\n",
      "| id|  type|   amt|code|\n",
      "+---+------+------+----+\n",
      "|100| DEBIT|1000.0| IND|\n",
      "|101|CREDIT|2000.0| IND|\n",
      "|102| DEBIT|3000.0| AUS|\n",
      "|103|CREDIT|4000.0| JPN|\n",
      "|104| DEBIT|5000.0| IND|\n",
      "|105|CREDIT|6000.0| AUS|\n",
      "+---+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(100,\"DEBIT\",1000.0,\"IND\"),(101,\"CREDIT\",2000.0,\"IND\"),(102,\"DEBIT\",3000.0,\"AUS\"),\n",
    "                            (103,\"CREDIT\",4000.0,\"JPN\"),(104,\"DEBIT\",5000.0,\"IND\"),(105,\"CREDIT\",6000.0,\"AUS\")]\n",
    ", schema= 'id int, type string, amt float, code string')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e7b652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|   amt|lower(type)|\n",
      "+------+-----------+\n",
      "|1000.0|      debit|\n",
      "|2000.0|     credit|\n",
      "|3000.0|      debit|\n",
      "|4000.0|     credit|\n",
      "|5000.0|      debit|\n",
      "|6000.0|     credit|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, upper \n",
    "df.select(df.amt, lower(df.type)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16877fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|lower(type)|   amt|\n",
      "+-----------+------+\n",
      "|      debit|1000.0|\n",
      "|     credit|2000.0|\n",
      "|      debit|3000.0|\n",
      "|     credit|4000.0|\n",
      "|      debit|5000.0|\n",
      "|     credit|6000.0|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(lower(df.type),\"amt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e329b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8723b8c",
   "metadata": {},
   "source": [
    "To add dynamically individual column explicitly to the existing dataframe\n",
    "- dataframe.withColumn(Column_name, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1abdce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(100,\"DEBIT\",1000.0,\"IND\"),(101,\"CREDIT\",2000.0,\"IND\"),(102,\"DEBIT\",3000.0,\"AUS\"),\n",
    "                            (103,\"CREDIT\",4000.0,\"JPN\"),(104,\"DEBIT\",5000.0,\"IND\"),(105,\"CREDIT\",6000.0,\"AUS\")]\n",
    "\n",
    "columns = 'id int, type string, amt float, code string'\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "df_with_ts = df.withColumn(\"curr_timestamp\", current_timestamp())\n",
    "\n",
    "df_with_ts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('type').sum('amt').show() # if amt is not passed all the integer columns are displayed\n",
    "df.groupby('type').min('amt').show()\n",
    "df.groupby('type').max('amt').show()\n",
    "df.groupby('type').avg('amt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a90c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.type.like(\"%CREDIT%\") & (df.amt > 1000.0))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2239a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ts.filter(df_with_ts(\"curr_timestamp\").lt(lit(\"2023-02-28 16:55:22.725605\")))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe81956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efab778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "df.withColumn(\"desc\", concat(\"type\", lit(\" \"), \"amt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df.withColumn(\"desc\", concat_ws(\" \", \"type\", \"amt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928801c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58192da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
