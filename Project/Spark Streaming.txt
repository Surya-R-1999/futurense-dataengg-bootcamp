Batch processing vs Stream Processing:
--------------------------------------

Stream processing -> Real time processing (velocity of data plays a vital role) -> Stock Market Analysis , Movie Rating...

Batch Processing -> Stored data Processing

* The Stream processing is performed in various tools such as Spark Streaming, Apache Storm, Apache FLink, Apache KafKa.

	- Based on Performance and Architecture, Apache Storm perform's well.

	Drawback : 
	--------
		
		- Can handle only Streaming Data's. Doesn't Support with historical Data's.

	- Apache Spark can handle both Streaming and historical Data i.e. Both batch and real time processing referred as Lambda Architecture.

	- Apache Spark performs extremely well in case of Lambda Architecture. Apache Flink and Apache Druid are also capable to perform Lambda Architecture.  

	- Apache Hadoop perform's well in Batch Processing. 

	- Each tools perform's well in specific requirements in a distributed Manner.


* Spark STreaming:
---------------------

	- Apache Kafka (Initially used as Messaging System) :

		- In Messaging different types of Tools are available like Active MQ , Rabbit MQ , IBM MQ , Oracle MQ ......

		- Kafka enables the distributed messaging, so that it can handle huge volume of data.

		- If the source generates 2000 events per second, but Stream processing is capable to process 1000 events per second. So Kafka act's as intermediate between source and 
Stream processing tool which stores the data or buffers the data and sends the stream processing in a distributed manner.

		- Also it stores the resultant data of Stream Processing engine and sends the data in a distrbuted manner to storage tools.

		- Flume act's as intermediate tool between IOt (SOurce engine) and Kafka. 

* Working of Spark STreaming:
-----------------------------

	- If the source generates 2000 events per second, Spark Streaming doesn't perform real time processing in the same rate, instead it creates micro batches based on the event size or event time. (referred as window intervals)

	- Spark STreaming converts the continuous data to discrete data as Micro Batches based on time or size of events.

	- Storm won't create micro batches, but in case of lambda architecture, Spark performs well.

	- Every Micro Batch is called as D-Streams or discretized Streams. (Every D-Streams will be created as RDD.)

	- D-STreams is created continuously, and RDD is created for every time intervals.

	- DSTream -> RDD @ time 1        ->   RDD @ time 2    ->   RDD @ time 3     ->   RDD @ time 4

	       data at time 0 to 1        data at time 1 to 2    data at time 2 to 3    data at time 3 to 4

	- DSTreams are created based on the time. So every Dstreams will have different data present inside it as per the time.

	- And these RDD's are processed in same way as regular RDD transformations and Actions.


* Example:
----------

Terminal 1:
-----------

nc -lk 9999

Once the ssc start's In terminal 2, we have to give inputs here, based on the window interval it executes the output in terminal2 until ssc stop's.



Terminal 2:
-------------

# Dstreams can't perform all operations as RDD.

from pyspark.streaming import StreamingContext

ssc = StreamingContext(sc, 10)

ssc
<pyspark.streaming.context.StreamingContext object at 0x7faa4b5c3370>

lines = ssc.socketTextStream("localhost", 9999)

lines
<pyspark.streaming.dstream.DStream object at 0x7faa4b5c3d90>


counts = lines.flatMap(lambda line: line.split(" "))\
                  .map(lambda word: (word, 1))\
                  .reduceByKey(lambda a, b: a+b)


ssc.start() -> ACtual Stream Processing STarts.

 
ssc.stop()


Inference:
-----------

- Here the Input is fetched from Socket. 

- Input Source can be any other sources also.

- Mappers and Reducers are used to find the word counts of the input as per the window time.

- Once the ssc is stopped, we have to create a new Streaming Context.

- Since it is a streaming data, we have to manually start and stop the ssc.

